{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c66fde77-967a-42f1-9a04-18bfa14c8cb9",
   "metadata": {},
   "source": [
    "# Email Spam Detection using RNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74df1c84-ef3a-4eec-8d6a-002ffa38d6dd",
   "metadata": {},
   "source": [
    "## 1. Import all used packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df675903-c5fc-43f4-a964-8b50e66b2195",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/davidanggawijaya/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "2023-01-28 02:55:02.439843: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "import nltk\n",
    "import ssl\n",
    "import re\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences \n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Bidirectional\n",
    "from keras.models import Model\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee35219-36e8-4dbd-a064-372ceef8a577",
   "metadata": {},
   "source": [
    "## 2. importing email spam dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "674dae79-17b3-44fb-b435-3ebeea1f54e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Subject: enron methanol ; meter # : 988291\\r\\n...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Subject: hpl nom for january 9 , 2001\\r\\n( see...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Subject: neon retreat\\r\\nho ho ho , we ' re ar...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Subject: photoshop , windows , office . cheap ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Subject: re : indian springs\\r\\nthis deal is t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  spam\n",
       "0  Subject: enron methanol ; meter # : 988291\\r\\n...     0\n",
       "1  Subject: hpl nom for january 9 , 2001\\r\\n( see...     0\n",
       "2  Subject: neon retreat\\r\\nho ho ho , we ' re ar...     0\n",
       "3  Subject: photoshop , windows , office . cheap ...     1\n",
       "4  Subject: re : indian springs\\r\\nthis deal is t...     0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"spam_ham_dataset.csv\")\n",
    "df.drop(['Unnamed: 0', 'label'], axis=1, inplace=True)\n",
    "df = df.rename(columns = {\"label_num\":\"spam\"})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d708e5-cbe6-4f49-86bd-1cccc26ec37d",
   "metadata": {},
   "source": [
    "## 3. cleaning the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6555073b-6f8e-499e-b51f-1eab39ea1b0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Subject enron methanol meter 988291 follow not...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Subject hpl nom january 9 2001 see attached fi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Subject neon retreat ho ho ho around wonderful...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Subject photoshop windows office cheap main tr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Subject indian springs deal book teco pvr reve...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  spam\n",
       "0  Subject enron methanol meter 988291 follow not...     0\n",
       "1  Subject hpl nom january 9 2001 see attached fi...     0\n",
       "2  Subject neon retreat ho ho ho around wonderful...     0\n",
       "3  Subject photoshop windows office cheap main tr...     1\n",
       "4  Subject indian springs deal book teco pvr reve...     0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove duplicate\n",
    "df.drop_duplicates(inplace = True)\n",
    "\n",
    "# clean each raw data lower case, no punctuation, no stop words\n",
    "def clean_text(text):\n",
    "    emoj = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "    text = re.sub(emoj, '', text)\n",
    "    nopunc = [char for char in text if char not in string.punctuation]\n",
    "    nopunc = ''.join(nopunc)\n",
    "    clean_words = [word for word in nopunc.split() if word.lower() not in stopwords.words(\"english\")]\n",
    "    return \" \".join(clean_words)\n",
    "\n",
    "df[\"text\"] = df[\"text\"].map(clean_text)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b236ff2d-9b61-4ae6-aefa-cc47f6d94b4c",
   "metadata": {},
   "source": [
    "## 4. split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "faa3c9e9-fe23-46bd-a73f-0d990ba24b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "emails_train, emails_test, target_train, target_test = train_test_split(df[\"text\"],df[\"spam\"],test_size = 0.2, random_state= 0) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b263a9-6d0b-44c8-844e-5a5867932711",
   "metadata": {},
   "source": [
    "## 5. tokenizing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3e04c4e-d0c7-4b30-836f-144d0687af9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## some config values \n",
    "embed_size = 100 # how big is each word vector\n",
    "max_feature = 50000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "max_len = 2000 # max number of words in a question to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31a31432-6a77-44b4-aa2f-069fae0f3f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 1256, 4334, 1546, 118, 2596, 10141, 10142, 1294, 1146, 62, 20043, 1238, 396, 381, 135, 569, 517, 4833, 1295, 13157, 133, 118, 2917, 411, 510, 57, 425, 133, 4833, 1295, 252, 2597, 50, 3158, 269, 1321, 293, 76, 792, 188, 547, 2262, 20044, 20045, 20046, 20047, 20048, 13158, 8480, 10143, 20049, 10144, 20050, 20051, 13159, 10145, 13160, 6050, 13161]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sb/xdj25f490m9_b8x517tyqfsc0000gn/T/ipykernel_68567/367788022.py:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  emails_train_tokens = np.array(tokenizer.texts_to_sequences(emails_train))\n",
      "/var/folders/sb/xdj25f490m9_b8x517tyqfsc0000gn/T/ipykernel_68567/367788022.py:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  emails_test_tokens = np.array(tokenizer.texts_to_sequences(emails_test))\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=max_feature)\n",
    "\n",
    "tokenizer.fit_on_texts(emails_train)\n",
    "\n",
    "emails_train_tokens = np.array(tokenizer.texts_to_sequences(emails_train))\n",
    "emails_test_tokens = np.array(tokenizer.texts_to_sequences(emails_test))\n",
    "print(emails_train_tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0231490c-f5df-43c7-bcd2-b0027e4f85c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     0,     0, ..., 13160,  6050, 13161], dtype=int32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalised the array\n",
    "emails_train_tokens = pad_sequences(emails_train_tokens,maxlen=max_len)\n",
    "emails_test_tokens = pad_sequences(emails_test_tokens,maxlen=max_len)\n",
    "emails_train_tokens[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7060cc-1e34-42cf-9302-c37b222b0c19",
   "metadata": {},
   "source": [
    "## 6. Creating RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae215962-3894-4db5-9204-cd4d196b11c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-28 02:56:02.120217: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 2000, 32)          1600000   \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 128)              49664     \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16)                2064      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 16)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,651,745\n",
      "Trainable params: 1,651,745\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "embedding_vecor_length = 32\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(Embedding(max_feature, embedding_vecor_length, input_length=max_len))\n",
    "model.add(Bidirectional(tf.keras.layers.LSTM(64)))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8ec596-cbd9-4f4e-8665-c16856660bcb",
   "metadata": {},
   "source": [
    "## 7. train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2dae7b5-2b6e-4ef9-b542-7e55cb427af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "8/8 [==============================] - 52s 6s/step - loss: 0.6661 - accuracy: 0.6998 - val_loss: 0.6167 - val_accuracy: 0.7227\n",
      "Epoch 2/20\n",
      "8/8 [==============================] - 56s 7s/step - loss: 0.6010 - accuracy: 0.7033 - val_loss: 0.5526 - val_accuracy: 0.7227\n",
      "Epoch 3/20\n",
      "8/8 [==============================] - 56s 7s/step - loss: 0.5317 - accuracy: 0.7033 - val_loss: 0.4377 - val_accuracy: 0.7227\n",
      "Epoch 4/20\n",
      "8/8 [==============================] - 62s 8s/step - loss: 0.4349 - accuracy: 0.7183 - val_loss: 0.3903 - val_accuracy: 0.7818\n",
      "Epoch 5/20\n",
      "5/8 [=================>............] - ETA: 23s - loss: 0.3440 - accuracy: 0.8414"
     ]
    }
   ],
   "source": [
    "#training stage\n",
    "history = model.fit(emails_train_tokens, target_train, batch_size=512, epochs=20, validation_data=(emails_test_tokens, target_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516ffbdc-7175-4989-955d-b4d50cfc4dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from  matplotlib import pyplot as plt\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.grid()\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e161e200-6a65-4822-8744-e86a93ae697f",
   "metadata": {},
   "source": [
    "## 8. analyst the training result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365f59b1-8d0a-4ea7-9dfd-cf660f8f38f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax= plt.subplot()\n",
    "spam_predict  = [1 if o>0.5 else 0 for o in model.predict(emails_test_tokens)]\n",
    "cf_matrix =confusion_matrix(target_test,spam_predict)\n",
    "sns.heatmap(cf_matrix, annot=True, ax = ax,cmap='Blues',fmt=''); #annot=True to annotate cells\n",
    "\n",
    "# labels, title and ticks\n",
    "ax.set_xlabel('Predicted labels');\n",
    "ax.set_ylabel('True labels'); \n",
    "ax.set_title('Confusion Matrix'); \n",
    "ax.xaxis.set_ticklabels(['Not Spam', 'Spam']); ax.yaxis.set_ticklabels(['Not Spam', 'Spam']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49ecc79-fbee-4a22-a4ff-0e4d686d7746",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(target_test,spam_predict))\n",
    "print()\n",
    "print(\"Accuracy: \", accuracy_score(target_test,spam_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511059f6-1ca3-4295-80ab-0ff7e4631a3b",
   "metadata": {},
   "source": [
    "## 9. testing stage using real life gmail emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f609c483-a285-42c6-b6e2-f9e48e63938e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df2 = pd.read_csv(\"label_spam.csv\")\n",
    "df2 = df2.rename(columns = {\"Email Text\":\"text\"})\n",
    "df2[\"text\"] = df2['Subject'] + \" \" + df2['text']\n",
    "df2.drop(['From', 'Subject'], axis=1, inplace=True)\n",
    "df2[\"spam\"] = 1\n",
    "df2[\"text\"] = df2[\"text\"].map(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433b8a01-6df1-467a-a942-b00718a1610e",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_emails_tokens = np.array(tokenizer.texts_to_sequences(df2[\"text\"]))\n",
    "real_emails_tokens = pad_sequences(real_emails_tokens,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a4e98f-4fc5-4d23-ab36-a448bcfdae8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction  = [1 if o>0.5 else 0 for o in model.predict(real_emails_tokens)]\n",
    "df2[\"prediction\"] = prediction\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a866b3-7a9f-488f-a2b5-5b3cde8f6d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy of predicting real life gmail: \", accuracy_score(prediction,df2[\"spam\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d42c9a-ee4f-4e69-bd18-3540a94b10da",
   "metadata": {},
   "source": [
    "## Conclusion: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bae11a-db09-4809-bfdd-5c7d00d17d65",
   "metadata": {},
   "source": [
    "1. overfitting the dataset fail to perform good in predicting a different dataset in this case the gmail's spam label\n",
    "2. dataset shift problem, The dataset that is used in the experiment is generated from few years ago, while the new dataset may no longer follow the same pattern due to socioeconomic factors such as spammers or attacker behaviour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd9d40b-94f9-4774-8721-ed6defcb0c40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
